{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import PyPDF2\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OCR Functions for Text Extraction\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extract text from a PDF file.\"\"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "        return text\n",
    "\n",
    "def extract_text_from_image(image_path):\n",
    "    \"\"\"Extract text from an image using OCR.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    text = pytesseract.image_to_string(image)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information Extraction Function\n",
    "def extract_invoice_info(text):\n",
    "    \"\"\"Extract information from an invoice using regular expressions.\"\"\"\n",
    "    info = {}\n",
    "    info['Invoice Number'] = re.search(r'order id\\s*:\\s*(\\d+)', text, re.IGNORECASE).group(1) if re.search(r'order id\\s*:\\s*(\\d+)', text, re.IGNORECASE) else \"Not found\"\n",
    "    info['Invoice Date'] = re.search(r'order date\\s*:\\s*(\\d{4}-\\d{2}-\\d{2})', text, re.IGNORECASE).group(1) if re.search(r'order date\\s*:\\s*(\\d{4}-\\d{2}-\\d{2})', text, re.IGNORECASE) else \"Not found\"\n",
    "    info['Customer ID'] = re.search(r'customer id\\s*:\\s*(\\w+)', text, re.IGNORECASE).group(1) if re.search(r'customer id\\s*:\\s*(\\w+)', text, re.IGNORECASE) else \"Not found\"\n",
    "    info['Total Amount'] = re.search(r'total price\\s*(\\d+\\.\\d+)', text, re.IGNORECASE).group(1) if re.search(r'total price\\s*(\\d+\\.\\d+)', text, re.IGNORECASE) else \"Not found\"\n",
    "    info['Contact Name'] = re.search(r'contact name\\s*:\\s*([A-Za-z\\s]+)', text, re.IGNORECASE).group(1).strip() if re.search(r'contact name\\s*:\\s*([A-Za-z\\s]+)', text, re.IGNORECASE) else \"Not found\"\n",
    "    info['Address'] = re.search(r'address\\s*:\\s*([A-Za-z0-9\\s,]+)', text, re.IGNORECASE).group(1).strip() if re.search(r'address\\s*:\\s*([A-Za-z0-9\\s,]+)', text, re.IGNORECASE) else \"Not found\"\n",
    "    info['City'] = re.search(r'city\\s*:\\s*([A-Za-z\\s]+)', text, re.IGNORECASE).group(1).strip() if re.search(r'city\\s*:\\s*([A-Za-z\\s]+)', text, re.IGNORECASE) else \"Not found\"\n",
    "    info['Postal Code'] = re.search(r'postal code\\s*:\\s*(\\d{4,5}-\\d{3})', text, re.IGNORECASE).group(1) if re.search(r'postal code\\s*:\\s*(\\d{4,5}-\\d{3})', text, re.IGNORECASE) else \"Not found\"\n",
    "    info['Country'] = re.search(r'country\\s*:\\s*([A-Za-z]+)', text, re.IGNORECASE).group(1).strip() if re.search(r'country\\s*:\\s*([A-Za-z]+)', text, re.IGNORECASE) else \"Not found\"\n",
    "    info['Phone'] = re.search(r'phone\\s*:\\s*\\(?\\d{2}\\)?\\s*\\d{3}-\\d{4}', text, re.IGNORECASE).group(0).replace('phone:', '').strip() if re.search(r'phone\\s*:\\s*\\(?\\d{2}\\)?\\s*\\d{3}-\\d{4}', text, re.IGNORECASE) else \"Not found\"\n",
    "    info['Fax'] = re.search(r'fax\\s*:\\s*\\(?\\d{2}\\)?\\s*\\d{3}-\\d{4}', text, re.IGNORECASE).group(0).replace('fax:', '').strip() if re.search(r'fax\\s*:\\s*\\(?\\d{2}\\)?\\s*\\d{3}-\\d{4}', text, re.IGNORECASE) else \"Not found\"\n",
    "\n",
    "    products = []\n",
    "    product_lines = re.findall(r'(\\d+)\\s+([A-Za-z\\s]+)\\s+(\\d+)\\s+(\\d+\\.\\d+)', text, re.IGNORECASE)\n",
    "    for product in product_lines:\n",
    "        products.append({\n",
    "            'Product ID': product[0],\n",
    "            'Product Name': product[1].strip(),\n",
    "            'Quantity': product[2],\n",
    "            'Unit Price': product[3]\n",
    "        })\n",
    "    info['Products'] = products if products else \"Not found\"\n",
    "\n",
    "    return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model Definition (Functional)\n",
    "def create_cnn_model(num_classes):\n",
    "    \"\"\"Create a CNN model using functional layers.\"\"\"\n",
    "    model = nn.Sequential(\n",
    "        nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1),  # Grayscale input\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(32 * 56 * 56, 128),  # Assuming 224x224 input images\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(0.5),\n",
    "        nn.Linear(128, num_classes)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Training and Evaluation\n",
    "def train_cnn(image_dir, num_epochs=10, batch_size=32, learning_rate=0.001):\n",
    "    \"\"\"Train the CNN for document classification.\"\"\"\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    # Load dataset using ImageFolder\n",
    "    dataset = datasets.ImageFolder(image_dir, transform=transform)\n",
    "    train_size = int(0.8 * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Create model\n",
    "    num_classes = len(dataset.classes)\n",
    "    model = create_cnn_model(num_classes)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(\"Training CNN for Document Classification:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    invoice_correct = 0\n",
    "    invoice_total = 0\n",
    "    class_names = dataset.classes\n",
    "    invoice_idx = class_names.index('invoice') if 'invoice' in class_names else -1\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            # Track accuracy for invoices specifically\n",
    "            if invoice_idx != -1:\n",
    "                invoice_mask = (labels == invoice_idx)\n",
    "                invoice_total += invoice_mask.sum().item()\n",
    "                invoice_correct += (predicted[invoice_mask] == labels[invoice_mask]).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total if total > 0 else 0\n",
    "    invoice_accuracy = 100 * invoice_correct / invoice_total if invoice_total > 0 else 0\n",
    "    \n",
    "    print(\"CNN Analysis:\")\n",
    "    print(f\"Overall Test Accuracy: {accuracy}%\")\n",
    "    print(f\"Invoice Recognition Accuracy: {invoice_accuracy}%\")\n",
    "    print(\"Class Names:\", class_names)\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"../models/cnn_model.pth\")\n",
    "    return model, class_names, transform\n",
    "\n",
    "def predict_document_type(image_path, model, class_names, transform):\n",
    "    \"\"\"Predict the document type of a single image.\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    image = image.to(device)\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_class = class_names[predicted.item()]\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-trained model found. Training a new CNN...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mcnn_pipeline\u001b[39m\u001b[34m(image_path, input_type, image_dir)\u001b[39m\n\u001b[32m      8\u001b[39m model = create_cnn_model(num_classes)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../models/cnn_model.pth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     10\u001b[39m class_names = [\u001b[33m'\u001b[39m\u001b[33minvoice\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mshipping_order\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mpurchase_order\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mreport\u001b[39m\u001b[33m'\u001b[39m]  \u001b[38;5;66;03m# Adjust based on your dataset\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STATS_FINAL/.venv/lib/python3.12/site-packages/torch/serialization.py:1425\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1423\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1425\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1426\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1427\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1428\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1429\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STATS_FINAL/.venv/lib/python3.12/site-packages/torch/serialization.py:751\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    750\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STATS_FINAL/.venv/lib/python3.12/site-packages/torch/serialization.py:732\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../models/cnn_model.pth'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m image_path = \u001b[33m\"\u001b[39m\u001b[33m../data/images/invoice/0.png\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# Adjust this path to an actual image\u001b[39;00m\n\u001b[32m     50\u001b[39m input_type = \u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m category, extracted_info = \u001b[43mcnn_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mcnn_pipeline\u001b[39m\u001b[34m(image_path, input_type, image_dir)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo pre-trained model found. Training a new CNN...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     model, class_names, transform = \u001b[43mtrain_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Step 2: Classify the document type\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m input_type == \u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mtrain_cnn\u001b[39m\u001b[34m(image_dir, num_epochs, batch_size, learning_rate)\u001b[39m\n\u001b[32m      5\u001b[39m transform = transforms.Compose([\n\u001b[32m      6\u001b[39m     transforms.Grayscale(num_output_channels=\u001b[32m1\u001b[39m),\n\u001b[32m      7\u001b[39m     transforms.Resize((\u001b[32m224\u001b[39m, \u001b[32m224\u001b[39m)),\n\u001b[32m      8\u001b[39m     transforms.ToTensor(),\n\u001b[32m      9\u001b[39m     transforms.Normalize((\u001b[32m0.5\u001b[39m,), (\u001b[32m0.5\u001b[39m,))\n\u001b[32m     10\u001b[39m ])\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load dataset using ImageFolder\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m dataset = \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m train_size = \u001b[38;5;28mint\u001b[39m(\u001b[32m0.8\u001b[39m * \u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[32m     15\u001b[39m test_size = \u001b[38;5;28mlen\u001b[39m(dataset) - train_size\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STATS_FINAL/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:328\u001b[39m, in \u001b[36mImageFolder.__init__\u001b[39m\u001b[34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[39m\n\u001b[32m    319\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    320\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    321\u001b[39m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m   (...)\u001b[39m\u001b[32m    326\u001b[39m     allow_empty: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    327\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m     \u001b[38;5;28mself\u001b[39m.imgs = \u001b[38;5;28mself\u001b[39m.samples\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STATS_FINAL/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:149\u001b[39m, in \u001b[36mDatasetFolder.__init__\u001b[39m\u001b[34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    139\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    140\u001b[39m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[32m   (...)\u001b[39m\u001b[32m    146\u001b[39m     allow_empty: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    147\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    148\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(root, transform=transform, target_transform=target_transform)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     classes, class_to_idx = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m     samples = \u001b[38;5;28mself\u001b[39m.make_dataset(\n\u001b[32m    151\u001b[39m         \u001b[38;5;28mself\u001b[39m.root,\n\u001b[32m    152\u001b[39m         class_to_idx=class_to_idx,\n\u001b[32m   (...)\u001b[39m\u001b[32m    155\u001b[39m         allow_empty=allow_empty,\n\u001b[32m    156\u001b[39m     )\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mself\u001b[39m.loader = loader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STATS_FINAL/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:234\u001b[39m, in \u001b[36mDatasetFolder.find_classes\u001b[39m\u001b[34m(self, directory)\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) -> Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m    208\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[32m    209\u001b[39m \n\u001b[32m    210\u001b[39m \u001b[33;03m        directory/\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    232\u001b[39m \u001b[33;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[32m    233\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/STATS_FINAL/.venv/lib/python3.12/site-packages/torchvision/datasets/folder.py:41\u001b[39m, in \u001b[36mfind_classes\u001b[39m\u001b[34m(directory)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) -> Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[32m     37\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[32m     38\u001b[39m \n\u001b[32m     39\u001b[39m \u001b[33;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     classes = \u001b[38;5;28msorted\u001b[39m(entry.name \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry.is_dir())\n\u001b[32m     42\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[32m     43\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../data/images'"
     ]
    }
   ],
   "source": [
    "# CNN Pipeline: Classification + Information Extraction\n",
    "def cnn_pipeline(image_path, input_type, image_dir=\"../data/images\"):\n",
    "    \"\"\"Run the entire CNN pipeline: classify document type and extract information.\"\"\"\n",
    "    # Step 1: Train the CNN (or load a pre-trained model)\n",
    "    try:\n",
    "        # Check if a pre-trained model exists\n",
    "        num_classes = 4  # Adjust based on your dataset (e.g., invoice, shipping_order, purchase_order, report)\n",
    "        model = create_cnn_model(num_classes)\n",
    "        model.load_state_dict(torch.load(\"../models/cnn_model.pth\"))\n",
    "        class_names = ['invoice', 'shipping_order', 'purchase_order', 'report']  # Adjust based on your dataset\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,))\n",
    "        ])\n",
    "        print(\"Loaded pre-trained CNN model.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No pre-trained model found. Training a new CNN...\")\n",
    "        model, class_names, transform = train_cnn(image_dir, num_epochs=10)\n",
    "\n",
    "    # Step 2: Classify the document type\n",
    "    if input_type == 'image':\n",
    "        category = predict_document_type(image_path, model, class_names, transform)\n",
    "    elif input_type == 'pdf':\n",
    "        # For PDFs, we need to convert to an image first (simplified for this example)\n",
    "        raise NotImplementedError(\"PDF input requires conversion to image. Please provide an image file.\")\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported input type. Use 'image' or 'pdf'.\")\n",
    "\n",
    "    print(\"CNN Pipeline Analysis:\")\n",
    "    print(f\"Predicted Category: {category}\")\n",
    "\n",
    "    # Step 3: Extract information if the document is an invoice\n",
    "    if category.lower() == 'invoice':\n",
    "        # Extract text using OCR\n",
    "        text = extract_text_from_image(image_path)\n",
    "        print(\"Extracted Text:\\n\", text)\n",
    "        \n",
    "        # Extract structured information\n",
    "        extracted_info = extract_invoice_info(text)\n",
    "        print(\"Extracted Information:\", extracted_info)\n",
    "        return category, extracted_info\n",
    "    else:\n",
    "        print(\"Document is not an invoice. Skipping information extraction.\")\n",
    "        return category, None\n",
    "\n",
    "# Run the pipeline on a sample image\n",
    "image_path = \"../data/images/invoice/0.png\"  # Adjust this path to an actual image\n",
    "input_type = 'image'\n",
    "category, extracted_info = cnn_pipeline(image_path, input_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Image\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(f\"Predicted Category: {category}\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
